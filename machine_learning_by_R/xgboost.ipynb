{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting methods in R\n",
    "### by Xiaoqi Zheng, 0724/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rsample)      # data splitting \n",
    "library(gbm)          # basic implementation\n",
    "library(xgboost)      # a faster implementation of gbm\n",
    "library(caret)        # an aggregator package for performing many machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 5 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Private</th><th scope=col>Apps</th><th scope=col>Accept</th><th scope=col>Enroll</th><th scope=col>Top10perc</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Abilene Christian University</th><td>Yes</td><td>1660</td><td>1232</td><td>721</td><td>23</td></tr>\n",
       "\t<tr><th scope=row>Adelphi University</th><td>Yes</td><td>2186</td><td>1924</td><td>512</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>Adrian College</th><td>Yes</td><td>1428</td><td>1097</td><td>336</td><td>22</td></tr>\n",
       "\t<tr><th scope=row>Agnes Scott College</th><td>Yes</td><td> 417</td><td> 349</td><td>137</td><td>60</td></tr>\n",
       "\t<tr><th scope=row>Alaska Pacific University</th><td>Yes</td><td> 193</td><td> 146</td><td> 55</td><td>16</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Private & Apps & Accept & Enroll & Top10perc\\\\\n",
       "  & <fct> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tAbilene Christian University & Yes & 1660 & 1232 & 721 & 23\\\\\n",
       "\tAdelphi University & Yes & 2186 & 1924 & 512 & 16\\\\\n",
       "\tAdrian College & Yes & 1428 & 1097 & 336 & 22\\\\\n",
       "\tAgnes Scott College & Yes &  417 &  349 & 137 & 60\\\\\n",
       "\tAlaska Pacific University & Yes &  193 &  146 &  55 & 16\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 5\n",
       "\n",
       "| <!--/--> | Private &lt;fct&gt; | Apps &lt;dbl&gt; | Accept &lt;dbl&gt; | Enroll &lt;dbl&gt; | Top10perc &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| Abilene Christian University | Yes | 1660 | 1232 | 721 | 23 |\n",
       "| Adelphi University | Yes | 2186 | 1924 | 512 | 16 |\n",
       "| Adrian College | Yes | 1428 | 1097 | 336 | 22 |\n",
       "| Agnes Scott College | Yes |  417 |  349 | 137 | 60 |\n",
       "| Alaska Pacific University | Yes |  193 |  146 |  55 | 16 |\n",
       "\n"
      ],
      "text/plain": [
       "                             Private Apps Accept Enroll Top10perc\n",
       "Abilene Christian University Yes     1660 1232   721    23       \n",
       "Adelphi University           Yes     2186 1924   512    16       \n",
       "Adrian College               Yes     1428 1097   336    22       \n",
       "Agnes Scott College          Yes      417  349   137    60       \n",
       "Alaska Pacific University    Yes      193  146    55    16       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>777</li><li>18</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 777\n",
       "\\item 18\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 777\n",
       "2. 18\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 777  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load data\n",
    "library(tidyverse)\n",
    "library(ISLR)\n",
    "\n",
    "ml_data <- College\n",
    "ml_data[1:5,1:5]\n",
    "dim(ml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into training and test data\n",
    "set.seed(42)\n",
    "index <- createDataPartition(ml_data$Private, p = 0.7, list = FALSE)\n",
    "train_data <- ml_data[index, ]\n",
    "test_data  <- ml_data[-index, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stochastic Gradient Boosting \n",
       "\n",
       "545 samples\n",
       " 17 predictor\n",
       "  2 classes: 'No', 'Yes' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 3 times) \n",
       "Summary of sample sizes: 437, 436, 436, 435, 436, 436, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  interaction.depth  n.trees  Accuracy   Kappa    \n",
       "  1                   50      0.9369738  0.8366455\n",
       "  1                  100      0.9388143  0.8424288\n",
       "  1                  150      0.9443022  0.8584537\n",
       "  2                   50      0.9430959  0.8521284\n",
       "  2                  100      0.9449251  0.8583012\n",
       "  2                  150      0.9442967  0.8579690\n",
       "  3                   50      0.9381859  0.8399493\n",
       "  3                  100      0.9412385  0.8490957\n",
       "  3                  150      0.9485780  0.8686399\n",
       "\n",
       "Tuning parameter 'shrinkage' was held constant at a value of 0.1\n",
       "\n",
       "Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were n.trees = 150, interaction.depth =\n",
       " 3, shrinkage = 0.1 and n.minobsinnode = 10."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model with preprocessing & repeated cv\n",
    "model_gbm <- caret::train(Private ~ .,\n",
    "                          data = train_data,\n",
    "                          method = \"gbm\",\n",
    "                          trControl = trainControl(method = \"repeatedcv\", \n",
    "                                                  number = 5, \n",
    "                                                  repeats = 3, \n",
    "                                                  verboseIter = FALSE),\n",
    "                          verbose = 0)\n",
    "model_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  No Yes\n",
       "       No   56   6\n",
       "       Yes   7 163\n",
       "                                          \n",
       "               Accuracy : 0.944           \n",
       "                 95% CI : (0.9061, 0.9698)\n",
       "    No Information Rate : 0.7284          \n",
       "    P-Value [Acc > NIR] : <2e-16          \n",
       "                                          \n",
       "                  Kappa : 0.8577          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 1               \n",
       "                                          \n",
       "            Sensitivity : 0.8889          \n",
       "            Specificity : 0.9645          \n",
       "         Pos Pred Value : 0.9032          \n",
       "         Neg Pred Value : 0.9588          \n",
       "             Prevalence : 0.2716          \n",
       "         Detection Rate : 0.2414          \n",
       "   Detection Prevalence : 0.2672          \n",
       "      Balanced Accuracy : 0.9267          \n",
       "                                          \n",
       "       'Positive' Class : No              \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## test \n",
    "caret::confusionMatrix(data = predict(model_gbm, test_data),\n",
    "                       reference = test_data$Private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. eXtreme Gradient Boosting (XGboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "trctrl <- trainControl(method = \"cv\", number = 5)\n",
    "\n",
    "tune_grid <- expand.grid(nrounds = 140:150,\n",
    "                        max_depth = 5,\n",
    "                        eta = 0.05,\n",
    "                        gamma = 0.01,\n",
    "                        colsample_bytree = 0.75,\n",
    "                        min_child_weight = 0,\n",
    "                        subsample = 0.5)\n",
    "\n",
    "xgb.model <- train(Private ~ .,\n",
    "                data = train_data, \n",
    "                method = \"xgbTree\",\n",
    "                trControl=trctrl,\n",
    "                tuneGrid = tune_grid,\n",
    "                tuneLength = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eXtreme Gradient Boosting \n",
       "\n",
       "545 samples\n",
       " 17 predictor\n",
       "  2 classes: 'No', 'Yes' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 437, 435, 436, 436, 436 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  nrounds  Accuracy   Kappa    \n",
       "  140      0.9413156  0.8485455\n",
       "  141      0.9413156  0.8485455\n",
       "  142      0.9413156  0.8485455\n",
       "  143      0.9413156  0.8485455\n",
       "  144      0.9413156  0.8485455\n",
       "  145      0.9413156  0.8485455\n",
       "  146      0.9413156  0.8485455\n",
       "  147      0.9394807  0.8443698\n",
       "  148      0.9413156  0.8485455\n",
       "  149      0.9449686  0.8577046\n",
       "  150      0.9449686  0.8577046\n",
       "\n",
       "Tuning parameter 'max_depth' was held constant at a value of 5\n",
       "Tuning\n",
       "\n",
       "Tuning parameter 'min_child_weight' was held constant at a value of 0\n",
       "\n",
       "Tuning parameter 'subsample' was held constant at a value of 0.5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were nrounds = 149, max_depth = 5, eta\n",
       " = 0.05, gamma = 0.01, colsample_bytree = 0.75, min_child_weight = 0\n",
       " and subsample = 0.5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# have a look at the model \n",
    "xgb.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_predict <- predict(xgb.model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  No Yes\n",
       "       No   55   9\n",
       "       Yes   8 160\n",
       "                                          \n",
       "               Accuracy : 0.9267          \n",
       "                 95% CI : (0.8853, 0.9567)\n",
       "    No Information Rate : 0.7284          \n",
       "    P-Value [Acc > NIR] : 1.967e-14       \n",
       "                                          \n",
       "                  Kappa : 0.8157          \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 1               \n",
       "                                          \n",
       "            Sensitivity : 0.8730          \n",
       "            Specificity : 0.9467          \n",
       "         Pos Pred Value : 0.8594          \n",
       "         Neg Pred Value : 0.9524          \n",
       "             Prevalence : 0.2716          \n",
       "         Detection Rate : 0.2371          \n",
       "   Detection Prevalence : 0.2759          \n",
       "      Balanced Accuracy : 0.9099          \n",
       "                                          \n",
       "       'Positive' Class : No              \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caret::confusionMatrix(data = test_predict,\n",
    "                       reference = test_data$Private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see: https://www.hackerearth.com/zh/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
